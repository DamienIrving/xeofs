{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Canonical Correlation Analysis\n\nIn this example, we're going to perform a Canonical Correlation Analysis (CCA) \non three datasets using the ERSSTv5 monthly sea surface temperature (SST) data \nfrom 1970 to 2022. We divide this data into three areas: the Indian Ocean, \nthe Pacific Ocean, and the Atlantic Ocean. Our goal is to perform CCA on these \nregions.\n\nFirst, we'll import the necessary modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import xarray as xr\nimport xeofs as xe\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport cartopy.crs as ccrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we load the data and compute the SST anomalies. This removes the\nmonthly climatologies, so the seasonal cycle doesn't impact our CCA.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sst = xr.tutorial.load_dataset(\"ersstv5\").sst\nsst = sst.groupby(\"time.month\") - sst.groupby(\"time.month\").mean(\"time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we define the three regions of interest and store them in a list.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "indian = sst.sel(lon=slice(35, 115), lat=slice(30, -30))\npacific = sst.sel(lon=slice(130, 290), lat=slice(30, -30))\natlantic = sst.sel(lon=slice(320, 360), lat=slice(70, 10))\n\ndata_list = [indian, pacific, atlantic]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now perform CCA. Since we are dealing with a high-dimensional feature space, we first\nperform PCA to reduce the dimensionality (this is kind of a regularized CCA) by setting\n``pca=True``. By setting the ``variance_fraction`` keyword argument, we specify that we\nwant to keep the number of PCA modes that explain 90% of the variance in each of the\nthree data sets.\n\nAn important parameter is ``init_pca_modes``. It specifies the number\nof PCA modes that are initially compute before truncating them to account for 90 %. If this\nnumber is small enough, randomized PCAs will be performed instead of the full SVD decomposition\nwhich is much faster. We can also specify ``init_pca_modes`` as a float (0 < x <= 1),\nin which case the number of PCA modes is given by the fraction of the data matrix's rank\nThe default is set to 0.75 which will ensure that randomized PCAs are performed.\n\nGiven the nature of SST data, we might lower it to something like 0.3, since\nwe expect that most of the variance in the data will be explained by a small\nnumber of PC modes.\n\nNote that if our initial PCA modes don't hit the 90% variance target, ``xeofs``\nwill give a warning.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = xe.models.CCA(\n    n_modes=2,\n    use_coslat=True,\n    pca=True,\n    variance_fraction=0.9,\n    init_pca_modes=0.30,\n)\nmodel.fit(data_list, dim=\"time\")\ncomponents = model.components()\nscores = model.scores()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the canonical loadings (components) of the first mode.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mode = 1\n\ncentral_longitudes = [\n    indian.lon.median().item(),\n    pacific.lon.median().item(),\n    pacific.lon.median().item(),\n]\nprojections = [ccrs.PlateCarree(central_longitude=lon) for lon in central_longitudes]\n\nfig = plt.figure(figsize=(12, 2.5))\ngs = GridSpec(1, 4, figure=fig, width_ratios=[2, 4, 1, 0.2])\naxes = [fig.add_subplot(gs[0, i], projection=projections[i]) for i in range(3)]\ncax = fig.add_subplot(1, 4, 4)\nkwargs = dict(transform=ccrs.PlateCarree(), vmin=-1, vmax=1, cmap=\"RdBu_r\", cbar_ax=cax)\ncomponents[0].sel(mode=mode).plot(ax=axes[0], **kwargs)\ncomponents[1].sel(mode=mode).plot(ax=axes[1], **kwargs)\nim = components[2].sel(mode=mode).plot(ax=axes[2], **kwargs)\nfig.colorbar(im, cax=cax, orientation=\"vertical\")\nfor ax in axes:\n    ax.coastlines()\n    ax.set_title(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And lastly, we'll check out the canonical variates (scores) of the first mode.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 4))\nscores[0].sel(mode=mode).plot(ax=ax, label=\"Indian Ocean\")\nscores[1].sel(mode=mode).plot(ax=ax, label=\"Central Pacific\")\nscores[2].sel(mode=mode).plot(ax=ax, label=\"North Atlantic\")\nax.legend()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}